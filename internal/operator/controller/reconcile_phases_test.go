package controller

import (
	"context"
	"testing"

	hcloudgo "github.com/hetznercloud/hcloud-go/v2/hcloud"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/tools/record"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client/fake"

	k8znerv1alpha1 "github.com/imamik/k8zner/api/v1alpha1"
)

func TestReconcileInfrastructurePhase(t *testing.T) {
	t.Parallel()
	scheme := setupTestScheme(t)

	t.Run("skips creation when infrastructure already exists from CLI bootstrap", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Status: k8znerv1alpha1.K8znerClusterStatus{
				Infrastructure: k8znerv1alpha1.InfrastructureStatus{
					NetworkID:      100,
					LoadBalancerID: 200,
					FirewallID:     300,
					LoadBalancerIP: "1.2.3.4",
				},
			},
		}

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder,
			WithHCloudClient(&MockHCloudClient{}),
			WithMetrics(false),
		)

		result, err := r.reconcileInfrastructurePhase(context.Background(), cluster)
		require.NoError(t, err)
		assert.True(t, result.Requeue)
		assert.Equal(t, k8znerv1alpha1.PhaseImage, cluster.Status.ProvisioningPhase)
	})

	t.Run("sets control plane endpoint from LB IP when not set", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Status: k8znerv1alpha1.K8znerClusterStatus{
				ControlPlaneEndpoint: "", // not set
				Infrastructure: k8znerv1alpha1.InfrastructureStatus{
					NetworkID:      100,
					LoadBalancerID: 200,
					FirewallID:     300,
					LoadBalancerIP: "5.6.7.8",
				},
			},
		}

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder,
			WithHCloudClient(&MockHCloudClient{}),
			WithMetrics(false),
		)

		_, err := r.reconcileInfrastructurePhase(context.Background(), cluster)
		require.NoError(t, err)
		assert.Equal(t, "5.6.7.8", cluster.Status.ControlPlaneEndpoint)
	})

	t.Run("preserves existing control plane endpoint", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Status: k8znerv1alpha1.K8znerClusterStatus{
				ControlPlaneEndpoint: "existing-endpoint",
				Infrastructure: k8znerv1alpha1.InfrastructureStatus{
					NetworkID:      100,
					LoadBalancerID: 200,
					FirewallID:     300,
					LoadBalancerIP: "5.6.7.8",
				},
			},
		}

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder,
			WithHCloudClient(&MockHCloudClient{}),
			WithMetrics(false),
		)

		_, err := r.reconcileInfrastructurePhase(context.Background(), cluster)
		require.NoError(t, err)
		assert.Equal(t, "existing-endpoint", cluster.Status.ControlPlaneEndpoint)
	})

	t.Run("sets provisioning phase on entry", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Status: k8znerv1alpha1.K8znerClusterStatus{
				Infrastructure: k8znerv1alpha1.InfrastructureStatus{
					NetworkID:      1,
					LoadBalancerID: 2,
					FirewallID:     3,
				},
			},
		}

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder, WithMetrics(false))

		_, _ = r.reconcileInfrastructurePhase(context.Background(), cluster)
		assert.Equal(t, k8znerv1alpha1.ClusterPhaseProvisioning, cluster.Status.Phase)
	})

	t.Run("requeues on credentials error when fresh provisioning", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Spec: k8znerv1alpha1.K8znerClusterSpec{
				CredentialsRef: corev1.LocalObjectReference{
					Name: "missing-secret",
				},
			},
			// No infrastructure IDs â†’ will try to provision
		}

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder, WithMetrics(false))

		result, err := r.reconcileInfrastructurePhase(context.Background(), cluster)
		assert.NoError(t, err)
		assert.Equal(t, defaultRequeueAfter, result.RequeueAfter)
	})
}

func TestReconcileComputePhase(t *testing.T) {
	t.Parallel()
	scheme := setupTestScheme(t)

	t.Run("requeues on credentials error", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Spec: k8znerv1alpha1.K8znerClusterSpec{
				CredentialsRef: corev1.LocalObjectReference{
					Name: "missing-secret",
				},
			},
		}

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder, WithMetrics(false))

		result, err := r.reconcileComputePhase(context.Background(), cluster)
		assert.NoError(t, err)
		assert.Equal(t, defaultRequeueAfter, result.RequeueAfter)
	})
}

func TestReconcileImagePhase(t *testing.T) {
	t.Parallel()
	scheme := setupTestScheme(t)

	t.Run("requeues on credentials error", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Spec: k8znerv1alpha1.K8znerClusterSpec{
				CredentialsRef: corev1.LocalObjectReference{
					Name: "missing-secret",
				},
			},
		}

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder, WithMetrics(false))

		result, err := r.reconcileImagePhase(context.Background(), cluster)
		assert.NoError(t, err)
		assert.Equal(t, defaultRequeueAfter, result.RequeueAfter)
	})
}

func TestReconcileBootstrapPhase(t *testing.T) {
	t.Parallel()
	scheme := setupTestScheme(t)

	t.Run("requeues on credentials error", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Spec: k8znerv1alpha1.K8znerClusterSpec{
				CredentialsRef: corev1.LocalObjectReference{
					Name: "missing-secret",
				},
			},
		}

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder, WithMetrics(false))

		result, err := r.reconcileBootstrapPhase(context.Background(), cluster)
		assert.NoError(t, err)
		assert.Equal(t, defaultRequeueAfter, result.RequeueAfter)
	})
}

func TestReconcileConfiguringPhase(t *testing.T) {
	t.Parallel()
	scheme := setupTestScheme(t)

	t.Run("requeues on credentials error", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Spec: k8znerv1alpha1.K8znerClusterSpec{
				CredentialsRef: corev1.LocalObjectReference{
					Name: "missing-secret",
				},
			},
		}

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder, WithMetrics(false))

		result, err := r.reconcileConfiguringPhase(context.Background(), cluster)
		assert.NoError(t, err)
		assert.Equal(t, defaultRequeueAfter, result.RequeueAfter)
	})
}

func TestReconcileRunningPhase(t *testing.T) {
	t.Parallel()
	scheme := setupTestScheme(t)

	t.Run("performs health check and returns requeue", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Spec: k8znerv1alpha1.K8znerClusterSpec{
				ControlPlanes: k8znerv1alpha1.ControlPlaneSpec{Count: 1},
				Workers:       k8znerv1alpha1.WorkerSpec{Count: 1},
			},
			Status: k8znerv1alpha1.K8znerClusterStatus{
				Phase: k8znerv1alpha1.ClusterPhaseRunning,
			},
		}

		cpNode := createTestNode("cp-1", true, true)
		workerNode := createTestNode("worker-1", false, true)

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster, cpNode, workerNode).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder,
			WithHCloudClient(&MockHCloudClient{}),
			WithMetrics(false),
		)

		result, err := r.reconcileRunningPhase(context.Background(), cluster)
		require.NoError(t, err)
		assert.Equal(t, defaultRequeueAfter, result.RequeueAfter)
		assert.Equal(t, 1, cluster.Status.ControlPlanes.Ready)
		assert.Equal(t, 1, cluster.Status.Workers.Ready)
	})

	t.Run("returns error when health check fails", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
		}

		// Don't register the cluster as a status subresource - k8s fake client will error on status update
		// Actually, reconcileHealthCheck only lists nodes and updates the cluster in-memory
		// Health check itself shouldn't fail - it just lists nodes and processes them

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder,
			WithHCloudClient(&MockHCloudClient{}),
			WithMetrics(false),
		)

		result, err := r.reconcileRunningPhase(context.Background(), cluster)
		require.NoError(t, err)
		// With no nodes, health check succeeds (no unhealthy nodes found)
		assert.Equal(t, defaultRequeueAfter, result.RequeueAfter)
	})

	t.Run("updates cluster phase based on health", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Spec: k8znerv1alpha1.K8znerClusterSpec{
				ControlPlanes: k8znerv1alpha1.ControlPlaneSpec{Count: 3},
				Workers:       k8znerv1alpha1.WorkerSpec{Count: 2},
			},
			Status: k8znerv1alpha1.K8znerClusterStatus{
				Phase: k8znerv1alpha1.ClusterPhaseRunning,
				ControlPlanes: k8znerv1alpha1.NodeGroupStatus{
					Desired: 3,
				},
				Workers: k8znerv1alpha1.NodeGroupStatus{
					Desired: 2,
				},
			},
		}

		// Create 3 healthy CP nodes and 2 healthy workers
		cp1 := createTestNode("cp-1", true, true)
		cp2 := createTestNode("cp-2", true, true)
		cp3 := createTestNode("cp-3", true, true)
		w1 := createTestNode("worker-1", false, true)
		w2 := createTestNode("worker-2", false, true)

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster, cp1, cp2, cp3, w1, w2).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder,
			WithHCloudClient(&MockHCloudClient{}),
			WithMetrics(false),
		)

		result, err := r.reconcileRunningPhase(context.Background(), cluster)
		require.NoError(t, err)
		assert.Equal(t, defaultRequeueAfter, result.RequeueAfter)
		assert.Equal(t, 3, cluster.Status.ControlPlanes.Ready)
		assert.Equal(t, 2, cluster.Status.Workers.Ready)
		assert.Equal(t, k8znerv1alpha1.ClusterPhaseRunning, cluster.Status.Phase)
	})

	t.Run("handles scaling when CP count mismatch", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Spec: k8znerv1alpha1.K8znerClusterSpec{
				ControlPlanes: k8znerv1alpha1.ControlPlaneSpec{Count: 3},
				Workers:       k8znerv1alpha1.WorkerSpec{Count: 0},
			},
			Status: k8znerv1alpha1.K8znerClusterStatus{
				Phase: k8znerv1alpha1.ClusterPhaseRunning,
				ControlPlanes: k8znerv1alpha1.NodeGroupStatus{
					Desired: 3,
					Ready:   1,
					Nodes: []k8znerv1alpha1.NodeStatus{
						{Name: "cp-1", Healthy: true, PublicIP: "1.1.1.1"},
					},
				},
			},
		}

		cpNode := createTestNode("cp-1", true, true)

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster, cpNode).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		mockHCloud := &MockHCloudClient{
			GetSnapshotByLabelsFunc: func(ctx context.Context, labels map[string]string) (*hcloudgo.Image, error) {
				return &hcloudgo.Image{ID: 1}, nil
			},
		}

		r := NewClusterReconciler(fakeClient, scheme, recorder,
			WithHCloudClient(mockHCloud),
			WithMetrics(false),
		)

		result, err := r.reconcileRunningPhase(context.Background(), cluster)
		require.NoError(t, err)
		// reconcileControlPlanes will detect the mismatch and trigger scaling
		// The exact result depends on the scaling logic, but it should requeue
		assert.True(t, result.Requeue || result.RequeueAfter > 0,
			"should requeue for scaling or monitoring")
	})
}

func TestPhaseTransitionLogic(t *testing.T) {
	t.Parallel()

	t.Run("infrastructure phase transitions to Image on skip", func(t *testing.T) {
		t.Parallel()
		scheme := setupTestScheme(t)

		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Status: k8znerv1alpha1.K8znerClusterStatus{
				Infrastructure: k8znerv1alpha1.InfrastructureStatus{
					NetworkID:      1,
					LoadBalancerID: 2,
					FirewallID:     3,
				},
			},
		}

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder, WithMetrics(false))

		result, _ := r.reconcileInfrastructurePhase(context.Background(), cluster)
		assert.Equal(t, k8znerv1alpha1.PhaseImage, cluster.Status.ProvisioningPhase)
		assert.True(t, result.Requeue)
	})

	t.Run("infrastructure needs all three IDs to skip", func(t *testing.T) {
		t.Parallel()
		scheme := setupTestScheme(t)

		tests := []struct {
			name  string
			infra k8znerv1alpha1.InfrastructureStatus
			skips bool
		}{
			{
				name: "missing network ID",
				infra: k8znerv1alpha1.InfrastructureStatus{
					NetworkID:      0,
					LoadBalancerID: 2,
					FirewallID:     3,
				},
				skips: false,
			},
			{
				name: "missing LB ID",
				infra: k8znerv1alpha1.InfrastructureStatus{
					NetworkID:      1,
					LoadBalancerID: 0,
					FirewallID:     3,
				},
				skips: false,
			},
			{
				name: "missing firewall ID",
				infra: k8znerv1alpha1.InfrastructureStatus{
					NetworkID:      1,
					LoadBalancerID: 2,
					FirewallID:     0,
				},
				skips: false,
			},
			{
				name: "all present",
				infra: k8znerv1alpha1.InfrastructureStatus{
					NetworkID:      1,
					LoadBalancerID: 2,
					FirewallID:     3,
				},
				skips: true,
			},
		}

		for _, tt := range tests {
			t.Run(tt.name, func(t *testing.T) {
				t.Parallel()
				cluster := &k8znerv1alpha1.K8znerCluster{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "test-cluster",
						Namespace: "default",
					},
					Status: k8znerv1alpha1.K8znerClusterStatus{
						Infrastructure: tt.infra,
					},
				}

				fakeClient := fake.NewClientBuilder().
					WithScheme(scheme).
					WithObjects(cluster).
					WithStatusSubresource(cluster).
					Build()
				recorder := record.NewFakeRecorder(10)

				r := NewClusterReconciler(fakeClient, scheme, recorder, WithMetrics(false))

				result, _ := r.reconcileInfrastructurePhase(context.Background(), cluster)

				if tt.skips {
					assert.Equal(t, k8znerv1alpha1.PhaseImage, cluster.Status.ProvisioningPhase)
					assert.True(t, result.Requeue)
				} else {
					// Will fail at credentials loading (no secret), requeue
					assert.Equal(t, defaultRequeueAfter, result.RequeueAfter)
				}
			})
		}
	})
}

func TestReconcilePhases_EventRecording(t *testing.T) {
	t.Parallel()
	scheme := setupTestScheme(t)

	t.Run("infrastructure skip records event", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Status: k8znerv1alpha1.K8znerClusterStatus{
				Infrastructure: k8znerv1alpha1.InfrastructureStatus{
					NetworkID:      1,
					LoadBalancerID: 2,
					FirewallID:     3,
				},
			},
		}

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder, WithMetrics(false))

		_, _ = r.reconcileInfrastructurePhase(context.Background(), cluster)

		// Drain events and check
		select {
		case event := <-recorder.Events:
			assert.Contains(t, event, EventReasonInfrastructureCreated)
			assert.Contains(t, event, "CLI bootstrap")
		default:
			t.Error("expected event to be recorded")
		}
	})

	t.Run("running phase records no error events when healthy", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Spec: k8znerv1alpha1.K8znerClusterSpec{
				ControlPlanes: k8znerv1alpha1.ControlPlaneSpec{Count: 1},
				Workers:       k8znerv1alpha1.WorkerSpec{Count: 0},
			},
		}

		cpNode := createTestNode("cp-1", true, true)

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster, cpNode).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder,
			WithHCloudClient(&MockHCloudClient{}),
			WithMetrics(false),
		)

		_, err := r.reconcileRunningPhase(context.Background(), cluster)
		require.NoError(t, err)

		// No warning events expected
		select {
		case event := <-recorder.Events:
			assert.NotContains(t, event, "Warning")
		default:
			// No events is also fine
		}
	})
}

func TestReconcileMainSwitchDispatch(t *testing.T) {
	t.Parallel()
	scheme := setupTestScheme(t)

	// Test that the main reconcile function dispatches to the correct phase handler
	// based on the cluster's provisioning phase.

	t.Run("dispatches Running phase correctly", func(t *testing.T) {
		t.Parallel()
		cluster := &k8znerv1alpha1.K8znerCluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
			},
			Spec: k8znerv1alpha1.K8znerClusterSpec{
				ControlPlanes: k8znerv1alpha1.ControlPlaneSpec{Count: 1},
				Workers:       k8znerv1alpha1.WorkerSpec{Count: 0},
			},
			Status: k8znerv1alpha1.K8znerClusterStatus{
				Phase:             k8znerv1alpha1.ClusterPhaseRunning,
				ProvisioningPhase: k8znerv1alpha1.PhaseComplete,
			},
		}

		cpNode := createTestNode("cp-1", true, true)

		fakeClient := fake.NewClientBuilder().
			WithScheme(scheme).
			WithObjects(cluster, cpNode).
			WithStatusSubresource(cluster).
			Build()
		recorder := record.NewFakeRecorder(10)

		r := NewClusterReconciler(fakeClient, scheme, recorder,
			WithHCloudClient(&MockHCloudClient{}),
			WithMetrics(false),
		)

		result, err := r.Reconcile(context.Background(), ctrl.Request{
			NamespacedName: types.NamespacedName{
				Namespace: cluster.Namespace,
				Name:      cluster.Name,
			},
		})
		require.NoError(t, err)
		assert.Equal(t, defaultRequeueAfter, result.RequeueAfter)
	})
}
